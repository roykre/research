{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pnIRbEt6d0Z"
      },
      "source": [
        "#**Salesforce/blip-image-captioning-base**\n",
        "– BLIP’s base model (~400 MB). Produces 1–2 sentence captions on CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Salesforce/blip-image-captioning-base**\n",
        "– BLIP’s base model (~400 MB). Produces 1–2 sentence captions on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JBzr7MIVz2Nn",
        "outputId": "370e26a7-ac7c-409f-f7de-d5be5756f118"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch torchvision pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDU3dwU724_T",
        "outputId": "6e00b444-5198-4843-aecf-1b1bcb41703c"
      },
      "outputs": [],
      "source": [
        "# ──────────────── Imports ────────────────\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "import pandas as pd\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# ──────────────── Device Setup ────────────────\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ──────────────── Load BLIP Model & Processor ────────────────\n",
        "print(\"Loading BLIP...\")\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# ──────────────── Helper Functions ────────────────\n",
        "def load_image(source: str) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Load an image from a URL or local file path and return a PIL Image (RGB).\n",
        "    \"\"\"\n",
        "    if source.startswith(\"http\"):\n",
        "        return Image.open(requests.get(source, stream=True).raw).convert(\"RGB\")\n",
        "    else:\n",
        "        return Image.open(source).convert(\"RGB\")\n",
        "\n",
        "def run_blip(image: Image.Image, prompt: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Run the BLIP (base) model. If a prompt is provided, it will be used for prompted captioning.\n",
        "    After generation, any occurrence of the prompt text is removed from the returned caption.\n",
        "    \"\"\"\n",
        "    if prompt:\n",
        "        inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    else:\n",
        "        inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs)\n",
        "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt text from the generated caption if present\n",
        "    if prompt:\n",
        "        caption = caption.replace(prompt, \"\").strip()\n",
        "    return caption\n",
        "\n",
        "# ──────────────── Benchmark for BLIP Only ────────────────\n",
        "def benchmark_blip(image_source: str, prompts: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For a given image (URL or local path) and a list of prompts, run each prompt through\n",
        "    BLIP and record the captions (with prompt text removed) in a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    image = load_image(image_source)\n",
        "    records = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        record = {\n",
        "            \"Prompt\": prompt if prompt is not None else \"(no prompt)\",\n",
        "            \"BLIP_Caption\": run_blip(image, prompt),\n",
        "        }\n",
        "        records.append(record)\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# ──────────────── Example Usage ────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    image_url = \"test5.png\"\n",
        "    prompts = [\n",
        "        None,\n",
        "        \"Describe the scene.\",\n",
        "        \"what the man is doing?\",\n",
        "        \"What objects are in the image?\"\n",
        "    ]\n",
        "\n",
        "    df = benchmark_blip(image_url, prompts)\n",
        "    print(df.to_markdown(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**blip-image-captioning-large**\n",
        "– BLIP large (~1.2 GB). Generates 1–3 sentence captions; needs ~6 GB VRAM or ~3–4 GB RAM on CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHTjzGR7Knb"
      },
      "source": [
        "#**blip-image-captioning-large**\n",
        "– BLIP large (~1.2 GB). Generates 1–3 sentence captions; needs ~6 GB VRAM or ~3–4 GB RAM on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UaPx8b0H7RMb",
        "outputId": "52ab5430-045b-4be2-c2c9-34c31f346f0d"
      },
      "outputs": [],
      "source": [
        "# ───────────── Install Dependencies ─────────────\n",
        "!pip install -q transformers torch torchvision pillow pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "0e31c470f42640089942b54df58694bb",
            "f50d6f589a6f4c4ab148131591b5d6c8",
            "eca7e18cc4604dbc892665a835e9eb60",
            "ed4787b178db45b187af25942018a215",
            "26f6208b35b341079dedb24b55687e0c",
            "4fa5959f1d5b48fab8667f5253e5fcaa",
            "71dd0a5061884067bd3df3742e556a53",
            "abf0331f7d93475c9b735ce620ae3728",
            "ef3d52ba60b14752b90e2b12c4547211",
            "0acf3f90a94344979a03bdb713723176",
            "a588fd7337a540959da39d1e91983424",
            "138d51c786de49b490805c84a9a5ea1b",
            "1c3b78f2be394c5cbe75c42ed2df5915",
            "3628461219164b1e941f92c3f23dffa0",
            "6c3d78ec0cab43b982e270ba68da67ef",
            "9bf2345e2ec3495dbac273038b73783e",
            "6b684d1fcab54a68a6a2738b46148839",
            "1c86748ad26b489cb74ff040d4eb28df",
            "6357e47aa1254b2db77d5a96d5960343",
            "9dffabfeed7e4725ac4015d436fe9771",
            "c7c573d0b3604f468ee2ba825fab7b98",
            "01144e33e19d41558304d89bb48d04a4",
            "bfa9c29aff2c456a9c7a761455f89f79",
            "56b03132b1c445ea848984b64c3eb341",
            "95ee36b211594e91a6829e11c2aaed2e",
            "8dc37ac89b8f4fd8ae17ec945d840de0",
            "ea7b33ecde64443984e022aad9175471",
            "c9f1583662fb47f2a463367906e84d27",
            "fadc6ae7f76d4b219fc21745c78e5622",
            "586785d78f374384ba72672dcd06293f",
            "250f43eb50954f04b550fd7b746a822a",
            "e4c8a72adab34b35bf2f88a30b2b5468",
            "51fa3a993f8e47eba062b119faf6067c",
            "d9aef9e6f7db44fda4f1e3bd3490e66f",
            "0815491b7964455fbd95a6826a48b384",
            "f851dea242a34f3e82a5bdc9a3e9537a",
            "94ba2e92535d4328b8e8adf1b5daa7f9",
            "9bba533e9f604e71866df469e241fbd4",
            "b8a3e3509c0a44b1a5b27320bed49a7f",
            "2294f0d006c74c61b47d8c7403ae1f64",
            "59c7bac1ee2840e2a640de0b29e758dc",
            "7180bb7c97464bb5894c9382de957721",
            "bcead5ea4c2d4baaa452559b2a0d36bd",
            "18f3337cc0b542579564e573804ecaae",
            "5beb3db908fe4d39a3099aad60a33b29",
            "8e1bbcd752f14add99467c434655fca3",
            "ef99a3bb608c48d48b1c346c7c4022bd",
            "96f3a2ff3afc4fb1bbf1e12e9778dde1",
            "47221836c69246108cb9e69def1cf17d",
            "b9dfddab64c747af89246127e0e94e31",
            "5727fb01160849e691511b2ec65c675c",
            "be5b171dc0fc4375bd32426825b7963d",
            "fbeac10cfe25417093f1c6b99b424717",
            "1ffa0333f7b44b4fa83a330d7eb74ee8",
            "f6950aefba9c4e69b51f221a0c874662",
            "866e6317bfcc471da732cc0dc1f7dbc5",
            "5c18ac3079f54c338abd33e12569d4df",
            "c9a52afb00aa42fca735f466ec3d4ce0",
            "17084e82d9d1479d9cd1ef066208d5a2",
            "080a67d9d08546a498848ae1a62336ab",
            "8374c74f188149439ccd1524a8713a2f",
            "6b701c167b174577b9645dc07706f7e3",
            "1c736472745142ebacfa3178556583e9",
            "1608c510a1de4eec9e76fc0e4628b717",
            "4e1d949fcafd45eaa89926f282e073da",
            "b1b354d9058244f293e219f0006ea031",
            "f38a200a68b44e44b306ea3c6663b6e6",
            "bf3c05bb4d054d1586f9cc2ad4fe3c0c",
            "f8017638d40643be93e4265b88a4e719",
            "af5ed21be6a947db8322b8966fa7f5f0",
            "6f11b771885841c191844d9dea0d3c90",
            "e1364535ccd2437bb2628e90b5e311c7",
            "1202fb76afb943e6bb4ab61cfc7c59fb",
            "6e13ca8c3296423b8db58666a5f9bd6a",
            "3fc28b0aba884b94b5d5fd8cf2dd7fd9",
            "215985fbc28648569f42d1599fb462c7",
            "ae57562e51414cf7a855295f74b9442f"
          ]
        },
        "collapsed": true,
        "id": "m9HZTqBX7TXC",
        "outputId": "23087a8b-041a-4aa0-c95a-0c802a23c9ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ───────────── Imports ─────────────\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# ───────────── Device Setup ─────────────\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ───────────── Load BLIP-Large Model & Processor ─────────────\n",
        "print(\"Loading BLIP-Large...\")\n",
        "processor_large = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model_large     = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
        "\n",
        "# ───────────── Helper: Generate Captions ─────────────"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RrO2j8xu7J8P",
        "outputId": "1a70317c-d82f-42b8-e668-a5a06708dc89"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_blip_large_caption(image_path: str, prompt: str = None, max_new_tokens: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    If prompt is None, do unconditional caption. Otherwise feed prompt as prefix.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    if prompt is None:\n",
        "        inputs = processor_large(images=img, return_tensors=\"pt\").to(device)\n",
        "    else:\n",
        "        inputs = processor_large(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    output_ids = model_large.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=1)\n",
        "    raw = processor_large.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    # Strip prompt if model echoed it verbatim\n",
        "    if prompt and raw.lower().startswith(prompt.lower()):\n",
        "        return raw[len(prompt):].strip()\n",
        "    return raw\n",
        "\n",
        "# ───────────── Example Usage: Captions for Different Prompts ─────────────\n",
        "image_path = \"test5.png\"\n",
        "prompts = [\n",
        "    {\"label\": \"(no prompt)\",               \"query\": None},\n",
        "    {\"label\": \"Describe the scene.\",       \"query\": \"Describe the scene.\"},\n",
        "    {\"label\": \"What is the dog doing?\",     \"query\": \"What is the dog doing?\"},\n",
        "    {\"label\": \"What objects are in the image?\", \"query\": \"What objects are in the image?\"}\n",
        "]\n",
        "\n",
        "records = []\n",
        "for entry in prompts:\n",
        "    label = entry[\"label\"]\n",
        "    query = entry[\"query\"]\n",
        "    caption = generate_blip_large_caption(image_path, prompt=query, max_new_tokens=60)\n",
        "    records.append({\"Prompt\": label, \"BLIP-Large_Caption\": caption})\n",
        "\n",
        "df_large = pd.DataFrame(records)\n",
        "pd.set_option('display.max_colwidth', None)  # ensure full captions are visible\n",
        "\n",
        "# ───────────── Display Results as Markdown Table ─────────────\n",
        "print(\"Salesforce/blip-image-captioning-large\")\n",
        "print(\"– BLIP large model (~1.2 GB). Generates 1–3 sentence captions; GPU recommended.\\n\")\n",
        "print(df_large.to_markdown(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bTRtO25Uj2l"
      },
      "source": [
        "#**vit-gpt2-image-captioning**\n",
        "nlpconnect/vit-gpt2-image-captioning\n",
        "– ViT → GPT-2 (~370 MB). Also CPU-friendly; prompts via the VisionEncoderDecoder “decoder_input_ids” trick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**vit-gpt2-image-captioning**\n",
        "nlpconnect/vit-gpt2-image-captioning\n",
        "– ViT → GPT-2 (~370 MB). Also CPU-friendly; prompts via the VisionEncoderDecoder “decoder_input_ids” trick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QgWEujKkVonT",
        "outputId": "f00f6775-a301-413d-cc65-270fde18cac3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch pillow pandas requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357,
          "referenced_widgets": [
            "8c616e2e60224949a948bfdeec4e9c4a",
            "c6b5cc1d9f0a4d1ba7c8e7e12a5dadeb",
            "46124d6cff344906b3c4c43d595adbdc",
            "945c63c6dd824cdaa6f656556c3ec799",
            "926845b6d66b42ff9b7a2eadf0a3da7f",
            "9bd3c7068f2940928a7360c1aaa6ff03",
            "1e7f87d3b66248cfb13bf77950a30db4",
            "dc9bc45358134232a571b36ec91aa519",
            "a6abacadd368489485ef44de3b66a370",
            "d0a35e2fad6647b6874901611b9100b9",
            "0ab0d154720c431b9094a736106cab22",
            "58505a08d3b3468886d0952660eacdea",
            "8ef49321d9904ed999cc6f5aa3f6ad4c",
            "517cde3e6b00405fab1ebe8ec7da28b0",
            "e1282e5712614d4ba840fd3b066bcc1a",
            "6345592573d240789d9c2ff5c44cade6",
            "be05fe4ea4e0428bbebe7be9765e5d0a",
            "1d56aa83d0ce4c0fb0a8f3720e01733e",
            "3f7b933df0b1471ab303b896ae74f0d1",
            "9ec382e40a6443ebbdcaf18753e74314",
            "ad37d164e283478db2ced3e7dcb8953e",
            "098aee0c5889491b84c796d9763b15bf",
            "ebf1fcf9ddfd403d877a455dbea95c5f",
            "4128107e351c45b38d4401a24521e5c1",
            "17a6e319194d4dcb8efd7c54e2ff7dff",
            "935506f6c25c43cab0348285c54b9da5",
            "b75c61bb05da446a8a08076bffe72f13",
            "7940cfe22c444cb7b9a2a12d320af4c8",
            "0d84f247107e41baaf3b3ef8555cc967",
            "07b05f989ff848fe92905b1ea1d274e8",
            "26fa20e7976c46d9ba7687bfbd39ad06",
            "174476e749d242749e61da7dc5508be3",
            "c24d639e5f8148678e44a74ee3c1fbf0",
            "455c64e1d49f43b38ecdbcf75e27aafc",
            "c23ea8c085bd4623b876291226214520",
            "0ccf6a713008479cba7dae05cb8235ec",
            "7a58b889ac054723ab12c47adc0f6905",
            "3c8fd57841f24997aa12113920bd43ba",
            "87a742d028d54420a4504416afc487a2",
            "93557c79245e4cae9019d9ab87317e5f",
            "d694a2513f864946bc9a511038950a51",
            "3922e74337bf4c2999de6a72a142a191",
            "ee4399d1cec4492f920eb470b2d07106",
            "a6895614cee7462b866d333c980c8915",
            "04376e2973714757a56b51fc33853453",
            "14a176e47ee44651aaba1d6a9a20d2ab",
            "cc03521aff0247ab84239005b1cb11ae",
            "e660288c05314486a98847fd1c93f307",
            "57650643a20a40a69cca4366d42f4c63",
            "327f3dcc5f7c455b86f36d6bf3bb3fcb",
            "20df710bc39e43aaa1dd41845daf3ae2",
            "9e18ca7d49d44a3b84244dd1976e7bf7",
            "b1c03326c5224552afe10a6bcfb16b8c",
            "d72147dd525a44418dccf05c5cf99086",
            "4b1e3573a6f0407d84eb9ff3f4e618d6",
            "3fcecef6d8d8469289365be88455c085",
            "1de969f88afa4eda82492a46e850211f",
            "b681a97d2217431b9b2d1892af024f95",
            "225cddbfda614e358deddb8398d658a1",
            "3d0da7efa4394532aecf2103077312f5",
            "ff14987251c443e5b895a15260add725",
            "c146e80c836e4e139ed56d2ace28a3fc",
            "ce14420f024149a8b9d8be93f0d313f1",
            "2f159ddcbde4407ca7635f95d8e22a21",
            "bbfdb7f60dd342eeb6b827791eeb3cad",
            "2439a6fd71ba41a6a24512dd6a12b917",
            "aebd066338df4f92ba67b4af5c282f89",
            "0861d05f1ec44837973c67cda507733b",
            "b2b6eb44bf2e4efb945fa223627d8f61",
            "9beca2488e9e4e26a72529682b140f6b",
            "4acc4998569d43a4833267138ffbffa2",
            "dc27f379a8ce42f4ac148db597990a43",
            "875bc90c213642a3962178740dbc7182",
            "78e774036adc4c1b8eecd88a03099d20",
            "660a70ab0ab44fb99bb556c9246c0ca0",
            "66752b685cee46e5a5a09372df0183d1",
            "b5724961409e4eafba191b1075b9efe7",
            "1a0f574afe8249feaa0eb5407afd9c9d",
            "8c5640ff65674a27a0478467b3122914",
            "30cd69761c56476d9e773eec89bf7d4f",
            "fb69bfce7d134aafa5a8d25ca5c87692",
            "aa2671fd9d974385b3ff9a19bb5d5e73",
            "426f8b3d425e485ab38a4b0065ff8c81",
            "4f973970800b4e93b8d4d85581ecb310",
            "2169fcc1a5f344afb23fb6800d8a4e64",
            "f5002dc9fa50424d94d3a4ef90a2dab2",
            "3543d287ff86484fbd75d772fdd3a1d3",
            "38005165b105412eb048972750f111a6",
            "e34abbb82d49473d8d3ae782cafecfba",
            "b7aedb93570b4b6ba0661edf083c512c",
            "88426bdb4643428f962d438c49940217",
            "4efcfc97c6d147cfa54443f8fb073790",
            "ec5637175ccf4506aa5512ba78f117b4",
            "23069efb92dd4e3caa6b6c61766f1e33",
            "1c4358ce03ba4c61b663503c70feaa5e",
            "5578dd4de0614853ac0d67285d365711",
            "ab72ec3a31a548b888f196bbeea0576f",
            "ba7d3f87af2441c1a4ccf1a7d51237e5",
            "d118ad9d528e4d4ca3291f3b6a4cfa22"
          ]
        },
        "collapsed": true,
        "id": "gdssGlX8UmIw",
        "outputId": "edf659ab-4c54-474d-ee41-0fa57f42bb5b"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    ViTFeatureExtractor,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "from PIL import Image\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# ───────── Device Setup ─────────\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ───────── Load Model & Preprocessors ─────────\n",
        "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# (Make sure GPT-2 has a pad token defined; if not, set it equal to eos_token_id)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ekzpr4uHWxFv",
        "outputId": "eb4727db-91d5-438a-d624-e3dd53a5bb96"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ───────── Helper to Preprocess Image ─────────\n",
        "def prepare_image(image_path: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    1) Open PIL image, convert to RGB.\n",
        "    2) Run through ViTFeatureExtractor: returns pixel_values tensors.\n",
        "    3) Move to the correct device.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    encoded = feature_extractor(img, return_tensors=\"pt\")\n",
        "    return encoded[\"pixel_values\"].to(device)  # shape: (1, 3, H, W)\n",
        "\n",
        "# ───────── Unconditional Generation (no prompt) ─────────\n",
        "def generate_unconditional(image_path: str, max_length: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Generate a caption with NO text prompt (purely from image).\n",
        "    We set num_beams=1 to avoid the missing '_reorder_cache' error.\n",
        "    \"\"\"\n",
        "    pixel_values = prepare_image(image_path)\n",
        "    output_ids = model.generate(\n",
        "        pixel_values=pixel_values,\n",
        "        max_length=max_length,\n",
        "        num_beams=1,             # <— must be 1 to avoid beam‐search reorder cache\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    return text\n",
        "\n",
        "# ───────── Generate with a Text Prefix (“Prompt”) ─────────\n",
        "def generate_with_prompt(image_path: str, prompt_text: str, max_new_tokens: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    1) Preprocess image → pixel_values.\n",
        "    2) Tokenize prompt_text → decoder_input_ids.\n",
        "    3) Call model.generate(pixel_values=…, decoder_input_ids=…, num_beams=1).\n",
        "    4) Decode and strip away the prompt portion.\n",
        "    \"\"\"\n",
        "    pixel_values = prepare_image(image_path)\n",
        "    tokenized = tokenizer(\n",
        "        prompt_text,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "    decoder_input_ids = tokenized[\"input_ids\"].to(device)\n",
        "    output_ids = model.generate(\n",
        "        pixel_values=pixel_values,\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=1,             # <— set to 1 (greedy) instead of 4\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    # If GPT2 literally repeated our prompt_text at the front, strip it off:\n",
        "    if full_text.lower().startswith(prompt_text.lower()):\n",
        "        return full_text[len(prompt_text) :].strip()\n",
        "    return full_text\n",
        "\n",
        "# ─────────── Example: Print a Markdown Table ───────────\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"test5.png\"  # ← Replace with your actual file\n",
        "    prompts = [\n",
        "        None,\n",
        "        \"Describe the scene.\",\n",
        "        \"What is the man doing?\",\n",
        "        \"What objects are in the image?\"\n",
        "    ]\n",
        "    records = []\n",
        "    for p in prompts:\n",
        "        if p is None:\n",
        "            cap = generate_unconditional(image_path, max_length=50)\n",
        "            label = \"(no prompt)\"\n",
        "        else:\n",
        "            cap = generate_with_prompt(image_path, p, max_new_tokens=50)\n",
        "            label = p\n",
        "        records.append({\n",
        "            \"Prompt\": label,\n",
        "            \"VIT-GPT2_Caption\": cap\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rduugmN1BcRr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Open AI**\n",
        "\n",
        "by far the best preformance !\n",
        "\n",
        "right now set to \"gpt-4o-mini\" posiible to change it to other models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koY0bDPhYJgk"
      },
      "source": [
        "#**Open AI**\n",
        "\n",
        "by far the best preformance !\n",
        "\n",
        "right now set to \"gpt-4o-mini\" posiible to change it to other models\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y00OAAUZG7V"
      },
      "outputs": [],
      "source": [
        "# ───────────── Install Dependencies ─────────────\n",
        "!pip install -q openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pZmCBlFdYM8X",
        "outputId": "a0c6f3b9-bbfc-40af-b2c0-68fcaf1a983a"
      },
      "outputs": [],
      "source": [
        "# ───────────── Imports & API Key Setup ─────────────\n",
        "import openai\n",
        "import pandas as pd\n",
        "import base64 # Import base64 for image encoding\n",
        "import os # Import os to demonstrate environment variable method\n",
        "\n",
        "# Replace with your actual OpenAI API key\n",
        "# It's better practice to load API keys from environment variables\n",
        "# Ensure the OPENAI_API_KEY environment variable is set before running this.\n",
        "# If you must hardcode for this example, keep it here:\n",
        "\n",
        "## do not pubild this key !! @@@@\n",
        "api_key_value = \"sk-\"\n",
        "\n",
        "# Create an OpenAI client instance (new in v1.0.0+)\n",
        "# Pass the API key directly to the constructor\n",
        "# Alternatively, if using environment variables, you can remove the api_key=... part\n",
        "client = openai.OpenAI(api_key=api_key_value)\n",
        "\n",
        "\n",
        "# ───────────── Load Image Bytes and Encode ─────────────\n",
        "image_path = \"test4.png\"\n",
        "with open(image_path, \"rb\") as img_file:\n",
        "    image_bytes = img_file.read()\n",
        "\n",
        "# Encode the image bytes to base64 for inclusion in the API request\n",
        "base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "# ───────────── Define Prompts ─────────────\n",
        "prompt_entries = [\n",
        "   # {\"label\": \"(no prompt)\", \"query\": \"Describe the image.\"}\n",
        "    #{\"label\": \"Describe the scene.\", \"query\": \"Describe the scene.\"},\n",
        "    #{\"label\": \"What is the man doing?\", \"query\": \"What is the man doing?\"}\n",
        "    {\"label\": \"What objects are in the image?\", \"query\": \"What objects are in the image?\"}\n",
        "]\n",
        "\n",
        "# ───────────── Send Requests & Collect Responses ─────────────\n",
        "records = []\n",
        "for entry in prompt_entries:\n",
        "    try:\n",
        "        # Use the new client.chat.completions.create syntax\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",  # or another multimodal-capable model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": entry[\"query\"]},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
        "                            },\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "        )\n",
        "        # Access the content from the new response object structure\n",
        "        caption = response.choices[0].message.content.strip()\n",
        "        records.append({\"Prompt\": entry[\"label\"], \"OpenAI_Caption\": caption})\n",
        "    except Exception as e:\n",
        "        # Handle potential API errors more gracefully\n",
        "        print(f\"Error processing prompt '{entry['query']}': {e}\")\n",
        "        records.append({\"Prompt\": entry[\"label\"], \"OpenAI_Caption\": f\"Error: {e}\"})\n",
        "\n",
        "\n",
        "# ───────────── Display as Markdown Table ─────────────\n",
        "df = pd.DataFrame(records)\n",
        "print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9YvQpncaBOG"
      },
      "source": [
        "#**YOLO V8**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLO 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hYtjguC1OOBE",
        "outputId": "ed0499fa-4045-41e1-8696-92b792fad383"
      },
      "outputs": [],
      "source": [
        "# ───────────── Install Dependencies ─────────────\n",
        "!pip install -q ultralytics pandas pillow\n",
        "\n",
        "# ───────────── Imports & Model Setup ─────────────\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont # Import ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "model = YOLO(\"yolov8x.pt\")  # XL model, ~750 M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGLNc9niWUw_"
      },
      "source": [
        "plot original photo with boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "collapsed": true,
        "id": "15XB8Y2jOMcw",
        "outputId": "f779148c-38e6-41cb-c4b9-a7d28e2eea4d"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "# ───────────── Helper: Detect and Draw Boxes ─────────────\n",
        "def show_image_with_boxes(image_path: str):\n",
        "    \"\"\"\n",
        "    Runs YOLOv8x on the image, draws bounding boxes on the original,\n",
        "    and displays it inline.\n",
        "    \"\"\"\n",
        "    # 1) Load the image as a PIL Image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # 2) Run YOLOv8x inference\n",
        "    # Ensure 'model' is defined in the global scope or passed into the function\n",
        "    # For this context, assuming 'model = YOLO(\"yolov8x.pt\")' is run previously\n",
        "    results = model(image_path)[0]  # only one image\n",
        "\n",
        "    # 3) Create a drawing context and load a default font\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    try:\n",
        "        # Attempt to load a default font, handling potential exceptions\n",
        "        font = ImageFont.load_default()\n",
        "    except Exception:\n",
        "        # Fallback if load_default fails or is not available\n",
        "        print(\"Could not load default font, text drawing might be basic.\")\n",
        "        font = None # Or try loading a system font if necessary\n",
        "\n",
        "    # 4) If there are detections, overlay them\n",
        "    if hasattr(results, \"boxes\") and len(results.boxes) > 0:\n",
        "        for box in results.boxes:\n",
        "            # box.xyxy: tensor [[x1, y1, x2, y2]]\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().tolist()\n",
        "            # Confidence and label\n",
        "            conf = float(box.conf[0].cpu().numpy())\n",
        "            cls_id = int(box.cls[0].cpu().numpy())\n",
        "            label = model.names[cls_id]\n",
        "            # Draw rectangle\n",
        "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
        "\n",
        "            # Draw text background and label\n",
        "            text = f\"{label} {conf:.2f}\"\n",
        "\n",
        "            # --- FIX: Use textbbox instead of textsize ---\n",
        "            if font:\n",
        "                 # textbbox returns (left, top, right, bottom)\n",
        "                bbox = draw.textbbox((x1, y1), text, font=font)\n",
        "                text_width = bbox[2] - bbox[0]\n",
        "                text_height = bbox[3] - bbox[1]\n",
        "                # Draw background rectangle for text\n",
        "                draw.rectangle([x1, y1 - text_height, x1 + text_width, y1], fill=\"red\")\n",
        "                 # Draw text\n",
        "                draw.text((x1, y1 - text_height), text, fill=\"white\", font=font)\n",
        "            else:\n",
        "                 # Fallback if font loading failed - draw text without background\n",
        "                 # and potentially without size calculation if textlength is also unavailable\n",
        "                 try:\n",
        "                     # Use textlength as an alternative way to get width\n",
        "                     text_width = draw.textlength(text)\n",
        "                     # Approximate height (may need adjustment based on font size)\n",
        "                     approx_text_height = 15 # Example approximation\n",
        "                     draw.rectangle([x1, y1 - approx_text_height, x1 + text_width, y1], fill=\"red\")\n",
        "                     draw.text((x1, y1 - approx_text_height), text, fill=\"white\")\n",
        "                 except AttributeError:\n",
        "                      # If textlength is also not available (older Pillow or custom build)\n",
        "                      # Just draw the text directly at the top-left of the box\n",
        "                      print(f\"Warning: Could not calculate text size for '{text}'. Drawing text without background.\")\n",
        "                      draw.text((x1, y1), text, fill=\"white\")\n",
        "            # ---------------------------------------------\n",
        "\n",
        "    # 5) Display the resulting image\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show() # Ensure the plot is displayed\n",
        "\n",
        "# ───────────── Example Usage ─────────────\n",
        "image_path = \"test1.png\"  # Replace with your image file\n",
        "show_image_with_boxes(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4mOgXrEWaTx"
      },
      "source": [
        "print coordinate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KoECSjL2SIRd",
        "outputId": "0b4f0e7c-d2d5-4680-e546-10b13ad8ddd2"
      },
      "outputs": [],
      "source": [
        "# ───────────── Helper: Run YOLOv8x and Return Bounding Boxes ─────────────\n",
        "def yolo8x_detect_boxes(image_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs YOLOv8x on the given image and returns a DataFrame with:\n",
        "      - x1, y1, x2, y2 (pixel coords)\n",
        "      - confidence (float)\n",
        "      - class_id (int)\n",
        "      - label (string)\n",
        "    \"\"\"\n",
        "    # 1) Run inference; YOLO will handle loading the image internally\n",
        "    results = model(image_path)[0]  # index 0 for a single image\n",
        "\n",
        "    # 2) If no detections, return empty DataFrame\n",
        "    if not hasattr(results, \"boxes\") or len(results.boxes) == 0:\n",
        "        return pd.DataFrame(columns=[\"x1\", \"y1\", \"x2\", \"y2\", \"confidence\", \"class_id\", \"label\"])\n",
        "\n",
        "    # 3) Extract each box\n",
        "    data = []\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().tolist()\n",
        "        conf   = float(box.conf[0].cpu().numpy())\n",
        "        cls_id = int(box.cls[0].cpu().numpy())\n",
        "        label  = model.names[cls_id]\n",
        "        data.append({\n",
        "            \"x1\": x1,\n",
        "            \"y1\": y1,\n",
        "            \"x2\": x2,\n",
        "            \"y2\": y2,\n",
        "            \"confidence\": conf,\n",
        "            \"class_id\": cls_id,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# ───────────── Example Usage ─────────────\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"test3.png\"  # replace with your file\n",
        "    df_boxes = yolo8x_detect_boxes(image_path)\n",
        "    print(df_boxes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
